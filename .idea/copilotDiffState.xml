<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/.gitignore">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/.gitignore" />
              <option name="originalContent" value="# ...existing code...&#10;test_scraper.py&#10;config.py&#10;booktrack.db&#10;&#10;# Ignore PyCharm project files&#10;.idea/" />
              <option name="updatedContent" value="# ...existing code...&#10;test_scraper.py&#10;config.py&#10;booktrack.db&#10;&#10;# Ignore PyCharm project files&#10;.idea/" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/README.md" />
              <option name="originalContent" value="&#10;" />
              <option name="updatedContent" value="# BookTrack&#10;&#10;A basic Python application to scrape Amazon.com book listings and save results to a local SQLite database.&#10;&#10;## Structure&#10;&#10;- `main.py`: Entry point for the application. Scrapes Amazon and saves results to the database.&#10;- `scraper/`: Contains scraping logic.&#10;- `db/`: Contains database logic and export utilities.&#10;- `models/`: Data models (if needed).&#10;- `config_template.py`: Template for sensitive configuration variables. Copy to `config_private.py` and fill in your values.&#10;- `requirements.txt`: Python dependencies.&#10;&#10;## Configuration&#10;&#10;1. Copy `config_template.py` to `config_private.py` and fill in your values for `ITEM_ID` and `EXPORT_PATH`.&#10;2. Do **not** commit `config_private.py` to version control.&#10;&#10;## Usage&#10;&#10;1. Install dependencies:  &#10;   `pip install -r requirements.txt`&#10;&#10;2. Run the application to scrape and save a listing:  &#10;   `python main.py`&#10;&#10;3. To export all database entries to an XLSX file, run the export script:  &#10;   `python db/export_to_xls.py`  &#10;   The export path is set in your config file (`EXPORT_PATH`).&#10;&#10;## Notes&#10;&#10;- The export script (`db/export_to_xls.py`) is a stand-alone script. Run it whenever you want to generate an XLSX file from the current database contents.&#10;- The application uses Selenium with undetected-chromedriver for reliable scraping.&#10;- All sensitive configuration should be kept in `config_private.py` (not versioned)." />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/config.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/config.py" />
              <option name="originalContent" value="# The Amazon product/item ID to scrape. It is only tested for books. (string)&#10;ITEM_ID = &quot;0367724340&quot;&#10;&#10;# The path (absolute or relative) where the XLS export will be saved (string)&#10;EXPORT_PATH = &quot;/Users/kiesowd/Book_Tracking&quot;&#10;" />
              <option name="updatedContent" value="# The Amazon product/item ID to scrape. It is only tested for books. (string)&#10;ITEM_ID = &quot;0367724340&quot;&#10;&#10;# The path (absolute or relative) where the XLS export will be saved (string)&#10;EXPORT_PATH = &quot;/Users/kiesowd/Book_Tracking&quot;&#10;&#10;# The base URL for Amazon (change if scraping a different Amazon region)&#10;AMAZON_BASE_URL = &quot;https://www.amazon.com&quot;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/config_template.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/config_template.py" />
              <option name="updatedContent" value="# config_template.py&#10;# Copy this file to config_private.py and fill in your values.&#10;# Do NOT commit config_private.py to version control.&#10;&#10;# The Amazon product/item ID to scrape (string)&#10;ITEM_ID = &quot;YOUR_AMAZON_ITEM_ID&quot;&#10;&#10;# The path (absolute or relative) where the XLS export will be saved (string)&#10;EXPORT_PATH = &quot;booktrack_export.xlsx&quot;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/db/export_to_xls.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/db/export_to_xls.py" />
              <option name="originalContent" value="import os&#10;import sys&#10;import sqlite3&#10;import json&#10;from openpyxl import Workbook&#10;&#10;# Ensure the script can be run standalone by adjusting the import path if needed&#10;sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))&#10;from db.database import init_db, DB_PATH&#10;&#10;EXPORT_PATH = &quot;booktrack_export.xlsx&quot;&#10;&#10;def export_to_xls():&#10;    init_db()&#10;    conn = sqlite3.connect(DB_PATH)&#10;    c = conn.cursor()&#10;    c.execute(&quot;SELECT title, formats, best_sellers_rank, url, datestamp FROM listings&quot;)&#10;    rows = c.fetchall()&#10;    conn.close()&#10;&#10;    max_formats = 0&#10;    max_ranks = 0&#10;    parsed_rows = []&#10;    for row in rows:&#10;        title, formats, best_sellers_rank, url, datestamp = row&#10;        try:&#10;            formats_list = json.loads(formats) if formats else []&#10;        except Exception:&#10;            formats_list = []&#10;        try:&#10;            ranks_list = json.loads(best_sellers_rank) if best_sellers_rank else []&#10;        except Exception:&#10;            ranks_list = []&#10;        max_formats = max(max_formats, len(formats_list))&#10;        max_ranks = max(max_ranks, len(ranks_list))&#10;        parsed_rows.append((title, formats_list, ranks_list, url, datestamp))&#10;&#10;    wb = Workbook()&#10;    ws = wb.active&#10;    ws.title = &quot;Listings&quot;&#10;&#10;    header = [&quot;Title&quot;]&#10;    for i in range(max_formats):&#10;        header.append(f&quot;Format {i+1}&quot;)&#10;        header.append(f&quot;Price {i+1}&quot;)&#10;    for i in range(max_ranks):&#10;        header.append(f&quot;Category {i+1}&quot;)&#10;        header.append(f&quot;Rank {i+1}&quot;)&#10;    header.append(&quot;URL&quot;)&#10;    header.append(&quot;Datestamp&quot;)&#10;    ws.append(header)&#10;&#10;    for title, formats_list, ranks_list, url, datestamp in parsed_rows:&#10;        row = [title]&#10;        for i in range(max_formats):&#10;            if i &lt; len(formats_list):&#10;                fmt = formats_list[i]&#10;                row.append(fmt.get(&quot;format&quot;, &quot;&quot;) if isinstance(fmt, dict) else &quot;&quot;)&#10;                row.append(fmt.get(&quot;price&quot;, &quot;&quot;) if isinstance(fmt, dict) else &quot;&quot;)&#10;            else:&#10;                row.extend([&quot;&quot;, &quot;&quot;])&#10;        for i in range(max_ranks):&#10;            if i &lt; len(ranks_list):&#10;                rank = ranks_list[i]&#10;                row.append(rank.get(&quot;category&quot;, &quot;&quot;) if isinstance(rank, dict) else &quot;&quot;)&#10;                row.append(rank.get(&quot;rank&quot;, &quot;&quot;) if isinstance(rank, dict) else &quot;&quot;)&#10;            else:&#10;                row.extend([&quot;&quot;, &quot;&quot;])&#10;        row.append(url)&#10;        row.append(datestamp)&#10;        ws.append(row)&#10;&#10;    wb.save(EXPORT_PATH)&#10;    print(f&quot;[DB] Exported {len(rows)} rows to {EXPORT_PATH}&quot;)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    export_to_xls()&#10;" />
              <option name="updatedContent" value="import os&#10;import sys&#10;import sqlite3&#10;import json&#10;from openpyxl import Workbook&#10;&#10;# Ensure the script can be run standalone by adjusting the import path if needed&#10;sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))&#10;from db.database import init_db, DB_PATH&#10;from config import EXPORT_PATH&#10;&#10;def export_to_xls():&#10;    init_db()&#10;    conn = sqlite3.connect(DB_PATH)&#10;    c = conn.cursor()&#10;    c.execute(&quot;SELECT title, formats, best_sellers_rank, url, datestamp FROM listings&quot;)&#10;    rows = c.fetchall()&#10;    conn.close()&#10;&#10;    max_formats = 0&#10;    max_ranks = 0&#10;    parsed_rows = []&#10;    for row in rows:&#10;        title, formats, best_sellers_rank, url, datestamp = row&#10;        try:&#10;            formats_list = json.loads(formats) if formats else []&#10;        except Exception:&#10;            formats_list = []&#10;        try:&#10;            ranks_list = json.loads(best_sellers_rank) if best_sellers_rank else []&#10;        except Exception:&#10;            ranks_list = []&#10;        max_formats = max(max_formats, len(formats_list))&#10;        max_ranks = max(max_ranks, len(ranks_list))&#10;        parsed_rows.append((title, formats_list, ranks_list, url, datestamp))&#10;&#10;    wb = Workbook()&#10;    ws = wb.active&#10;    ws.title = &quot;Listings&quot;&#10;&#10;    header = [&quot;Title&quot;]&#10;    for i in range(max_formats):&#10;        header.append(f&quot;Format {i+1}&quot;)&#10;        header.append(f&quot;Price {i+1}&quot;)&#10;    for i in range(max_ranks):&#10;        header.append(f&quot;Category {i+1}&quot;)&#10;        header.append(f&quot;Rank {i+1}&quot;)&#10;    header.append(&quot;URL&quot;)&#10;    header.append(&quot;Datestamp&quot;)&#10;    ws.append(header)&#10;&#10;    for title, formats_list, ranks_list, url, datestamp in parsed_rows:&#10;        row = [title]&#10;        for i in range(max_formats):&#10;            if i &lt; len(formats_list):&#10;                fmt = formats_list[i]&#10;                row.append(fmt.get(&quot;format&quot;, &quot;&quot;) if isinstance(fmt, dict) else &quot;&quot;)&#10;                row.append(fmt.get(&quot;price&quot;, &quot;&quot;) if isinstance(fmt, dict) else &quot;&quot;)&#10;            else:&#10;                row.extend([&quot;&quot;, &quot;&quot;])&#10;        for i in range(max_ranks):&#10;            if i &lt; len(ranks_list):&#10;                rank = ranks_list[i]&#10;                row.append(rank.get(&quot;category&quot;, &quot;&quot;) if isinstance(rank, dict) else &quot;&quot;)&#10;                row.append(rank.get(&quot;rank&quot;, &quot;&quot;) if isinstance(rank, dict) else &quot;&quot;)&#10;            else:&#10;                row.extend([&quot;&quot;, &quot;&quot;])&#10;        row.append(url)&#10;        row.append(datestamp)&#10;        ws.append(row)&#10;&#10;    wb.save(EXPORT_PATH)&#10;    print(f&quot;[DB] Exported {len(rows)} rows to {EXPORT_PATH}&quot;)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    export_to_xls()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/main.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/main.py" />
              <option name="originalContent" value="from db.database import init_db, save_listing&#10;from config import ITEM_ID&#10;from scraper.book_collector import scrape_listing&#10;&#10;def main():&#10;    init_db()&#10;    url = f&quot;https://www.amazon.com/dp/{ITEM_ID}&quot;&#10;    print(f&quot;Scraping {url}&quot;)&#10;    data = scrape_listing(url)&#10;    save_listing(data)&#10;    print(&quot;Listing saved.&quot;)&#10;    print(&quot;Scraped data:&quot;)&#10;    print(data)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="from db.database import init_db, save_listing&#10;from config import ITEM_ID, AMAZON_BASE_URL&#10;from scraper.book_collector import scrape_listing&#10;&#10;def main():&#10;    init_db()&#10;    url = f&quot;{AMAZON_BASE_URL}/dp/{ITEM_ID}&quot;&#10;    print(f&quot;Scraping {url}&quot;)&#10;    data = scrape_listing(url)&#10;    save_listing(data)&#10;    print(&quot;Listing saved.&quot;)&#10;    print(&quot;Scraped data:&quot;)&#10;    print(data)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/scraper/amazon_scraper.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/scraper/amazon_scraper.py" />
              <option name="originalContent" value="import re&#10;from bs4 import BeautifulSoup&#10;import undetected_chromedriver as uc&#10;from selenium.webdriver.common.by import By&#10;from selenium.webdriver.support.ui import WebDriverWait&#10;from selenium.webdriver.support import expected_conditions as EC&#10;&#10;def parse_detail_bullets(detail_div):&#10;    parsed = []&#10;    def parse_ul(ul):&#10;        items = []&#10;        for li in ul.find_all(&quot;li&quot;, recursive=False):&#10;            label = None&#10;            value = None&#10;            label_span = li.find(&quot;span&quot;, class_=&quot;a-text-bold&quot;)&#10;            if label_span:&#10;                label = label_span.get_text(strip=True).rstrip(&quot;:&quot;)&#10;                label_span.extract()&#10;                value = li.get_text(strip=True)&#10;            else:&#10;                value = li.get_text(strip=True)&#10;            # Check for nested &lt;ul&gt;&#10;            sub_ul = li.find(&quot;ul&quot;, recursive=False)&#10;            if sub_ul:&#10;                subitems = parse_ul(sub_ul)&#10;                if label:&#10;                    items.append({label: {&quot;value&quot;: value, &quot;subitems&quot;: subitems}})&#10;                else:&#10;                    items.append({&quot;value&quot;: value, &quot;subitems&quot;: subitems})&#10;            else:&#10;                if label:&#10;                    items.append({label: value})&#10;                else:&#10;                    items.append(value)&#10;        return items&#10;&#10;    for ul in detail_div.find_all(&quot;ul&quot;, recursive=False):&#10;        parsed.extend(parse_ul(ul))&#10;    return parsed&#10;&#10;def find_details_div(header):&#10;    # Try to find the nearest ancestor &lt;div&gt;&#10;    ancestor = header.find_parent(&quot;div&quot;)&#10;    if ancestor:&#10;        return ancestor&#10;    # If not found, try next siblings&#10;    sibling = header.find_next_sibling()&#10;    while sibling:&#10;        if getattr(sibling, &quot;name&quot;, None) == &quot;div&quot;:&#10;            return sibling&#10;        sibling = sibling.find_next_sibling()&#10;    return None&#10;&#10;def scrape_listing(url):&#10;    # Use undetected-chromedriver to avoid bot detection&#10;    options = uc.ChromeOptions()&#10;    options.add_argument(&quot;--headless=new&quot;)&#10;    options.add_argument(&quot;--no-sandbox&quot;)&#10;    options.add_argument(&quot;--disable-gpu&quot;)&#10;    driver = uc.Chrome(options=options)&#10;    try:&#10;        driver.get(url)&#10;        # Wait for product title to load&#10;        WebDriverWait(driver, 10).until(&#10;            EC.presence_of_element_located((By.ID, &quot;productTitle&quot;))&#10;        )&#10;        html = driver.page_source&#10;    finally:&#10;        driver.quit()&#10;&#10;    soup = BeautifulSoup(html, &quot;html.parser&quot;)&#10;    # Title&#10;    title = soup.find(id=&quot;productTitle&quot;)&#10;&#10;    # Formats and prices: look for slot-title and slot-price pattern&#10;    formats = []&#10;    for slot_title in soup.find_all(&quot;span&quot;, class_=&quot;slot-title&quot;):&#10;        fmt = slot_title.get_text(strip=True)&#10;        price = None&#10;        # Look for adjacent slot-price span (next siblings)&#10;        slot_price = slot_title.find_next_sibling(&quot;span&quot;, class_=&quot;slot-price&quot;)&#10;        if slot_price:&#10;            price_span = slot_price.find(&quot;span&quot;)&#10;            if price_span:&#10;                price = price_span.get_text(strip=True)&#10;            else:&#10;                price = slot_price.get_text(strip=True)&#10;        formats.append({&#10;            &quot;format&quot;: fmt,&#10;            &quot;price&quot;: price&#10;        })&#10;    # Fallback to Northstar-Buybox logic if no slot-title found&#10;    if not formats:&#10;        def find_formats_prices(northstar_div):&#10;            found = []&#10;            for tag in northstar_div.find_all(['li', 'div', 'span', 'button'], recursive=True):&#10;                text = tag.get_text(&quot; &quot;, strip=True)&#10;                for fmt in [&quot;Kindle&quot;, &quot;Hardcover&quot;, &quot;Paperback&quot;]:&#10;                    if fmt.lower() in text.lower():&#10;                        price = None&#10;                        price_tag = tag.find(string=re.compile(r&quot;\$\d&quot;))&#10;                        if price_tag:&#10;                            price = price_tag.strip()&#10;                        else:&#10;                            price_match = re.search(r&quot;\$\d[\d,\.]*&quot;, text)&#10;                            if price_match:&#10;                                price = price_match.group(0)&#10;                        found.append({&#10;                            &quot;format&quot;: fmt,&#10;                            &quot;price&quot;: price&#10;                        })&#10;            return found&#10;&#10;        northstar_div = soup.find(&quot;div&quot;, id=re.compile(&quot;Northstar-Buybox&quot;, re.I))&#10;        if not northstar_div:&#10;            for div in soup.find_all(&quot;div&quot;):&#10;                if div.get(&quot;data-csa-c-type&quot;) and &quot;Northstar-Buybox&quot; in div.get(&quot;data-csa-c-type&quot;):&#10;                    northstar_div = div&#10;                    break&#10;                if &quot;northstar-buybox&quot; in &quot; &quot;.join(div.get(&quot;class&quot;, [])).lower():&#10;                    northstar_div = div&#10;                    break&#10;        if northstar_div:&#10;            formats = find_formats_prices(northstar_div)&#10;        else:&#10;            price = soup.find(&quot;span&quot;, {&quot;class&quot;: &quot;a-offscreen&quot;})&#10;            if price:&#10;                formats.append({&#10;                    &quot;format&quot;: &quot;Unknown&quot;,&#10;                    &quot;price&quot;: price.get_text(strip=True)&#10;                })&#10;&#10;    # Look for a header with &quot;Product Details&quot;&#10;    header = None&#10;    for tag in soup.find_all(['h1', 'h2', 'h3']):&#10;        if &quot;product details&quot; in tag.get_text(strip=True).lower():&#10;            header = tag&#10;            break&#10;&#10;    detail_div = None&#10;    if header:&#10;        detail_div = find_details_div(header)&#10;&#10;    if detail_div:&#10;        parsed_details = parse_detail_bullets(detail_div)&#10;        print(&quot;DEBUG: Parsed Product Details:&quot;)&#10;        for entry in parsed_details:&#10;            print(entry)&#10;    else:&#10;        parsed_details = []&#10;        print(&quot;DEBUG: Product Details header and containing &lt;div&gt; not found.&quot;)&#10;&#10;    # Best Sellers Rank extraction (unchanged)&#10;    ranks = []&#10;    if detail_div:&#10;        bsr_label = detail_div.find(&quot;span&quot;, class_=&quot;a-text-bold&quot;, string=re.compile(r&quot;Best Sellers Rank&quot;))&#10;        if bsr_label:&#10;            overall_text = bsr_label.next_sibling&#10;            if overall_text:&#10;                overall_match = re.search(r'#([\d,]+)\s+in\s+([^\(]+)', str(overall_text))&#10;                if overall_match:&#10;                    overall_rank = overall_match.group(1).replace(&quot;,&quot;, &quot;&quot;)&#10;                    overall_category = overall_match.group(2).strip()&#10;                    ranks.append({&#10;                        &quot;rank&quot;: overall_rank,&#10;                        &quot;category&quot;: overall_category&#10;                    })&#10;            for item in detail_div.find_all(&quot;span&quot;, class_=&quot;a-list-item&quot;):&#10;                cat_match = re.search(r'#([\d,]+)\s+in\s+([^\n\(]+)', item.get_text())&#10;                if cat_match:&#10;                    rank_clean = cat_match.group(1).replace(&quot;,&quot;, &quot;&quot;)&#10;                    category_clean = cat_match.group(2).strip()&#10;                    if not any(r[&quot;rank&quot;] == rank_clean and r[&quot;category&quot;] == category_clean for r in ranks):&#10;                        ranks.append({&#10;                            &quot;rank&quot;: rank_clean,&#10;                            &quot;category&quot;: category_clean&#10;                        })&#10;    return {&#10;        &quot;title&quot;: title.get_text(strip=True) if title else None,&#10;        &quot;formats&quot;: formats,&#10;        &quot;best_sellers_rank&quot;: ranks,&#10;        &quot;product_details&quot;: parsed_details,&#10;        &quot;url&quot;: url&#10;    }&#10;" />
              <option name="updatedContent" value="# (This file has been replaced by book_collector.py. You can remove it.)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/scraper/book_collector.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/scraper/book_collector.py" />
              <option name="updatedContent" value="import re&#10;from bs4 import BeautifulSoup&#10;import undetected_chromedriver as uc&#10;from selenium.webdriver.common.by import By&#10;from selenium.webdriver.support.ui import WebDriverWait&#10;from selenium.webdriver.support import expected_conditions as EC&#10;&#10;def parse_detail_bullets(detail_div):&#10;    parsed = []&#10;    def parse_ul(ul):&#10;        items = []&#10;        for li in ul.find_all(&quot;li&quot;, recursive=False):&#10;            label = None&#10;            value = None&#10;            label_span = li.find(&quot;span&quot;, class_=&quot;a-text-bold&quot;)&#10;            if label_span:&#10;                label = label_span.get_text(strip=True).rstrip(&quot;:&quot;)&#10;                label_span.extract()&#10;                value = li.get_text(strip=True)&#10;            else:&#10;                value = li.get_text(strip=True)&#10;            # Check for nested &lt;ul&gt;&#10;            sub_ul = li.find(&quot;ul&quot;, recursive=False)&#10;            if sub_ul:&#10;                subitems = parse_ul(sub_ul)&#10;                if label:&#10;                    items.append({label: {&quot;value&quot;: value, &quot;subitems&quot;: subitems}})&#10;                else:&#10;                    items.append({&quot;value&quot;: value, &quot;subitems&quot;: subitems})&#10;            else:&#10;                if label:&#10;                    items.append({label: value})&#10;                else:&#10;                    items.append(value)&#10;        return items&#10;&#10;    for ul in detail_div.find_all(&quot;ul&quot;, recursive=False):&#10;        parsed.extend(parse_ul(ul))&#10;    return parsed&#10;&#10;def find_details_div(header):&#10;    # Try to find the nearest ancestor &lt;div&gt;&#10;    ancestor = header.find_parent(&quot;div&quot;)&#10;    if ancestor:&#10;        return ancestor&#10;    # If not found, try next siblings&#10;    sibling = header.find_next_sibling()&#10;    while sibling:&#10;        if getattr(sibling, &quot;name&quot;, None) == &quot;div&quot;:&#10;            return sibling&#10;        sibling = sibling.find_next_sibling()&#10;    return None&#10;&#10;def scrape_listing(url):&#10;    # Use undetected-chromedriver to avoid bot detection&#10;    options = uc.ChromeOptions()&#10;    options.add_argument(&quot;--headless=new&quot;)&#10;    options.add_argument(&quot;--no-sandbox&quot;)&#10;    options.add_argument(&quot;--disable-gpu&quot;)&#10;    driver = uc.Chrome(options=options)&#10;    try:&#10;        driver.get(url)&#10;        # Wait for product title to load&#10;        WebDriverWait(driver, 10).until(&#10;            EC.presence_of_element_located((By.ID, &quot;productTitle&quot;))&#10;        )&#10;        html = driver.page_source&#10;    finally:&#10;        driver.quit()&#10;&#10;    soup = BeautifulSoup(html, &quot;html.parser&quot;)&#10;    # Title&#10;    title = soup.find(id=&quot;productTitle&quot;)&#10;&#10;    # Formats and prices: look for slot-title and slot-price pattern&#10;    formats = []&#10;    for slot_title in soup.find_all(&quot;span&quot;, class_=&quot;slot-title&quot;):&#10;        fmt = slot_title.get_text(strip=True)&#10;        price = None&#10;        # Look for adjacent slot-price span (next siblings)&#10;        slot_price = slot_title.find_next_sibling(&quot;span&quot;, class_=&quot;slot-price&quot;)&#10;        if slot_price:&#10;            price_span = slot_price.find(&quot;span&quot;)&#10;            if price_span:&#10;                price = price_span.get_text(strip=True)&#10;            else:&#10;                price = slot_price.get_text(strip=True)&#10;        formats.append({&#10;            &quot;format&quot;: fmt,&#10;            &quot;price&quot;: price&#10;        })&#10;    # Fallback to Northstar-Buybox logic if no slot-title found&#10;    if not formats:&#10;        def find_formats_prices(northstar_div):&#10;            found = []&#10;            for tag in northstar_div.find_all(['li', 'div', 'span', 'button'], recursive=True):&#10;                text = tag.get_text(&quot; &quot;, strip=True)&#10;                for fmt in [&quot;Kindle&quot;, &quot;Hardcover&quot;, &quot;Paperback&quot;]:&#10;                    if fmt.lower() in text.lower():&#10;                        price = None&#10;                        price_tag = tag.find(string=re.compile(r&quot;\$\d&quot;))&#10;                        if price_tag:&#10;                            price = price_tag.strip()&#10;                        else:&#10;                            price_match = re.search(r&quot;\$\d[\d,\.]*&quot;, text)&#10;                            if price_match:&#10;                                price = price_match.group(0)&#10;                        found.append({&#10;                            &quot;format&quot;: fmt,&#10;                            &quot;price&quot;: price&#10;                        })&#10;            return found&#10;&#10;        northstar_div = soup.find(&quot;div&quot;, id=re.compile(&quot;Northstar-Buybox&quot;, re.I))&#10;        if not northstar_div:&#10;            for div in soup.find_all(&quot;div&quot;):&#10;                if div.get(&quot;data-csa-c-type&quot;) and &quot;Northstar-Buybox&quot; in div.get(&quot;data-csa-c-type&quot;):&#10;                    northstar_div = div&#10;                    break&#10;                if &quot;northstar-buybox&quot; in &quot; &quot;.join(div.get(&quot;class&quot;, [])).lower():&#10;                    northstar_div = div&#10;                    break&#10;        if northstar_div:&#10;            formats = find_formats_prices(northstar_div)&#10;        else:&#10;            price = soup.find(&quot;span&quot;, {&quot;class&quot;: &quot;a-offscreen&quot;})&#10;            if price:&#10;                formats.append({&#10;                    &quot;format&quot;: &quot;Unknown&quot;,&#10;                    &quot;price&quot;: price.get_text(strip=True)&#10;                })&#10;&#10;    # Look for a header with &quot;Product Details&quot;&#10;    header = None&#10;    for tag in soup.find_all(['h1', 'h2', 'h3']):&#10;        if &quot;product details&quot; in tag.get_text(strip=True).lower():&#10;            header = tag&#10;            break&#10;&#10;    detail_div = None&#10;    if header:&#10;        detail_div = find_details_div(header)&#10;&#10;    if detail_div:&#10;        parsed_details = parse_detail_bullets(detail_div)&#10;    else:&#10;        parsed_details = []&#10;&#10;    # Best Sellers Rank extraction (unchanged)&#10;    ranks = []&#10;    if detail_div:&#10;        bsr_label = detail_div.find(&quot;span&quot;, class_=&quot;a-text-bold&quot;, string=re.compile(r&quot;Best Sellers Rank&quot;))&#10;        if bsr_label:&#10;            overall_text = bsr_label.next_sibling&#10;            if overall_text:&#10;                overall_match = re.search(r'#([\d,]+)\s+in\s+([^\(]+)', str(overall_text))&#10;                if overall_match:&#10;                    overall_rank = overall_match.group(1).replace(&quot;,&quot;, &quot;&quot;)&#10;                    overall_category = overall_match.group(2).strip()&#10;                    ranks.append({&#10;                        &quot;rank&quot;: overall_rank,&#10;                        &quot;category&quot;: overall_category&#10;                    })&#10;            for item in detail_div.find_all(&quot;span&quot;, class_=&quot;a-list-item&quot;):&#10;                cat_match = re.search(r'#([\d,]+)\s+in\s+([^\n\(]+)', item.get_text())&#10;                if cat_match:&#10;                    rank_clean = cat_match.group(1).replace(&quot;,&quot;, &quot;&quot;)&#10;                    category_clean = cat_match.group(2).strip()&#10;                    if not any(r[&quot;rank&quot;] == rank_clean and r[&quot;category&quot;] == category_clean for r in ranks):&#10;                        ranks.append({&#10;                            &quot;rank&quot;: rank_clean,&#10;                            &quot;category&quot;: category_clean&#10;                        })&#10;    return {&#10;        &quot;title&quot;: title.get_text(strip=True) if title else None,&#10;        &quot;formats&quot;: formats,&#10;        &quot;best_sellers_rank&quot;: ranks,&#10;        &quot;product_details&quot;: parsed_details,&#10;        &quot;url&quot;: url&#10;    }&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_scraper.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_scraper.py" />
              <option name="originalContent" value="import requests&#10;from bs4 import BeautifulSoup&#10;from config import ITEM_ID&#10;&#10;def main():&#10;    url = f&quot;https://www.amazon.com/dp/{ITEM_ID}&quot;&#10;    print(f&quot;Testing scrape for: {url}&quot;)&#10;    response = requests.get(url, headers={&quot;User-Agent&quot;: &quot;Mozilla/5.0&quot;})&#10;    soup = BeautifulSoup(response.text, &quot;html.parser&quot;)&#10;&#10;    parsed_lists = []&#10;    for ul in soup.find_all(&quot;ul&quot;):&#10;        parent_div = ul.find_parent(&quot;div&quot;)&#10;        div_tag = &quot;&quot;&#10;        if parent_div:&#10;            div_id = parent_div.get(&quot;id&quot;)&#10;            div_class = parent_div.get(&quot;class&quot;)&#10;            if div_id:&#10;                div_tag = f&quot;(div id='{div_id}')&quot;&#10;            elif div_class:&#10;                div_tag = f&quot;(div class='{' '.join(div_class)}')&quot;&#10;        items = [li.get_text(strip=True) for li in ul.find_all(&quot;li&quot;, recursive=False)]&#10;        parsed_lists.append({&#10;            &quot;container&quot;: div_tag,&#10;            &quot;items&quot;: items&#10;        })&#10;&#10;    print(&quot;\nParsed UL data:&quot;)&#10;    for entry in parsed_lists:&#10;        print(entry)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="import undetected_chromedriver as uc&#10;from selenium.webdriver.common.by import By&#10;from selenium.webdriver.support.ui import WebDriverWait&#10;from selenium.webdriver.support import expected_conditions as EC&#10;from bs4 import BeautifulSoup&#10;from config import ITEM_ID&#10;&#10;def main():&#10;    url = f&quot;https://www.amazon.com/dp/{ITEM_ID}&quot;&#10;    print(f&quot;Testing scrape for: {url}&quot;)&#10;&#10;    options = uc.ChromeOptions()&#10;    options.add_argument(&quot;--headless=new&quot;)&#10;    options.add_argument(&quot;--no-sandbox&quot;)&#10;    options.add_argument(&quot;--disable-gpu&quot;)&#10;    driver = uc.Chrome(options=options)&#10;    try:&#10;        driver.get(url)&#10;        WebDriverWait(driver, 10).until(&#10;            EC.presence_of_element_located((By.ID, &quot;productTitle&quot;))&#10;        )&#10;        html = driver.page_source&#10;    finally:&#10;        driver.quit()&#10;&#10;    soup = BeautifulSoup(html, &quot;html.parser&quot;)&#10;&#10;    parsed_lists = []&#10;    for ul in soup.find_all(&quot;ul&quot;):&#10;        parent_div = ul.find_parent(&quot;div&quot;)&#10;        div_tag = &quot;&quot;&#10;        if parent_div:&#10;            div_id = parent_div.get(&quot;id&quot;)&#10;            div_class = parent_div.get(&quot;class&quot;)&#10;            if div_id:&#10;                div_tag = f&quot;(div id='{div_id}')&quot;&#10;            elif div_class:&#10;                div_tag = f&quot;(div class='{' '.join(div_class)}')&quot;&#10;        items = [li.get_text(strip=True) for li in ul.find_all(&quot;li&quot;, recursive=False)]&#10;        parsed_lists.append({&#10;            &quot;container&quot;: div_tag,&#10;            &quot;items&quot;: items&#10;        })&#10;&#10;    print(&quot;\nParsed UL data:&quot;)&#10;    for entry in parsed_lists:&#10;        print(entry)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>